{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World\n"
     ]
    }
   ],
   "source": [
    "my_variable = \"Hello World\"\n",
    "print(my_variable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World\n"
     ]
    }
   ],
   "source": [
    "print(my_variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic testing using already existing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'NEGATIVE', 'score': 0.9997368454933167}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load a pre-trained sentiment analysis model\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "# Test with a review\n",
    "review = \"The movie was not ok.\"\n",
    "result = classifier(review)\n",
    "\n",
    "print(result)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "model_name = \"bert-base-uncased\"  # Pre-trained BERT model\n",
    "num_labels = 3  # Positive, Neutral, Negative\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your dataset (movie reviews + labels)\n",
    "movie_reviews = [\n",
    "    \"This movie was absolutely wonderful.\",  # Positive\n",
    "    \"I really loved the film!\",  # Positive\n",
    "    \"The movie was alright.\",  # Neutral\n",
    "    \"I didn't enjoy the film that much.\",  # Negative\n",
    "    \"This movie was not to my taste.\"  # Negative\n",
    "]\n",
    "\n",
    "labels = [2, 2, 1, 0, 0]  # Map labels: Negative=0, Neutral=1, Positive=2\n",
    "\n",
    "# Tokenize the dataset\n",
    "inputs = tokenizer(movie_reviews, padding=True, truncation=True, return_tensors=\"pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6 [00:00<?, ?it/s]<positron-console-cell-7>:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "100%|██████████| 6/6 [00:55<00:00,  9.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 55.0357, 'train_samples_per_second': 0.273, 'train_steps_per_second': 0.109, 'train_loss': 1.0507909456888835, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6, training_loss=1.0507909456888835, metrics={'train_runtime': 55.0357, 'train_samples_per_second': 0.273, 'train_steps_per_second': 0.109, 'total_flos': 92500810920.0, 'train_loss': 1.0507909456888835, 'epoch': 3.0})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MovieReviewDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "dataset = MovieReviewDataset(inputs, labels)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir=\"./logs\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following does not appear to work as desired as the model seems to always predict postive, more tinkering is required.  It can predict not positive options with drastic inputs like \"bad\" or direct inputs from the training like \"I didn't enjoy the film that much\".  Not sure if this is an issue with set up or evidence that more training is needed.  \n",
    "\n",
    "Upon further testing the model appears to be very different every time with different results being favoured.  Could suggest it just requires further training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Sentiment: Negative\n"
     ]
    }
   ],
   "source": [
    "new_review = [\"I really loved the film!\"]\n",
    "\n",
    "# Tokenize new review\n",
    "new_inputs = tokenizer(new_review, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Predict\n",
    "outputs = model(**new_inputs)\n",
    "predicted_class = torch.argmax(outputs.logits, dim=1).item()\n",
    "\n",
    "# Label mapping\n",
    "label_map = {0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"}\n",
    "print(f\"Predicted Sentiment: {label_map[predicted_class]}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
